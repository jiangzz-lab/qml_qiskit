{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import *\n",
    "from qiskit.circuit.library import GroverOperator\n",
    "from qiskit.quantum_info import Statevector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.visualization import plot_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroverQLearner:\n",
    "    ''' \n",
    "    Implement a quanutm reinforcement learning agent based on Grover amplitute enhancement and QLearing algorithm.\n",
    "\n",
    "    Assumption:\n",
    "    The dimensions of the state space and action space are both finite\n",
    "\n",
    "    Parameters:\n",
    "    env: the environment to solve; default is OpenAI gym \"FrozenLake\"\n",
    "    state (int): current state\n",
    "    action (int): current action \n",
    "    state_dimension (int): dimension of the state space\n",
    "    action_dimension (int): dimension of the action space\n",
    "    action_qregister_size (int): number of qubits on the quantum register for storing the action wavefunction \n",
    "    max_grover_length (int): maximum of the length of the grover iteration\n",
    "    Q_values (2D np array): Q values of all (state, action) combinations; shape = (state_dimension, action_dimention)\n",
    "    grover_lengths (2D np array): lengths of grover iterations of all (state, action) combinaitons; shape = (state_dimension, action_dimention)\n",
    "    grover_operators (1D np array): grover_operators for all actions; grover_operators[a] records the grover operator constructed from action eigenfucntion a \n",
    "    action_circuits (1D np array): action quantum circuits for all states; action_circuits[s] records the quanutm circuit encoding the up-to-date action wavefunction of state s\n",
    "    hyperparameters (dict): hyperparameters of learning; \n",
    "                            {\n",
    "                                'k': prefactor of max grover length, \n",
    "                                'alpha': learning rate, 'gamma': discount, \n",
    "                                'epsilonr': tolerance of the Q values,\n",
    "                                'max_epochs': max number of epochs for training,\n",
    "                                'max_steps': max number of steps in every epoch\n",
    "                            }\n",
    "    backend: machine to execute the quanutm circuit jobs; could be either a simulator or a true quantum computer\n",
    "    '''\n",
    "    def __init__(self, env) -> None:\n",
    "        self.env = env\n",
    "        self.state = env.reset()\n",
    "        self.action = 0\n",
    "        self.state_dimension = env.observation_space.n\n",
    "        self.action_dimension = env.action_space.n\n",
    "        self.action_qregister_size = ceil(np.log2(self.action_dimension))\n",
    "        self.max_grover_length = int(round(np.pi / (4 * np.arcsin(1. / np.sqrt(2 ** self.action_qregister_size))) - 0.5))\n",
    "        self.Q_values = np.zeros((self.state_dimension, self.action_dimension), dtype=float) \n",
    "        self.grover_length = np.zeros((self.state_dimension, self.action_dimension), dtype=int)\n",
    "        self.max_grover_length_reached = np.zeros((self.state_dimension, self.action_dimension), dtype=bool)\n",
    "        self.grover_operators = self._init_grover_operators()\n",
    "        self.action_circuits = self._init_action_circuits()\n",
    "        self.hyperparameters = {\n",
    "            'k': 0.1,\n",
    "            'alpha': 0.05,\n",
    "            'gamma': 0.99,\n",
    "            'eps': 0.01,\n",
    "            'max_epochs': 1000,\n",
    "            'max_steps': 100\n",
    "        }\n",
    "        self.backend = Aer.get_backend('qasm_simulator')\n",
    "            \n",
    "    # hyperparameter setter\n",
    "    def set_hyperparameters(self, params):\n",
    "        pass\n",
    "\n",
    "    def _init_action_circuits(self):\n",
    "        '''\n",
    "        Initialize the quanutm circuits encoding the action wavefunction of every state. Every initial action wavefunction is a equally weighted superposition of all action eignenfucntions. \n",
    "        '''\n",
    "        action_circuits = np.zeros(self.state_dimension)\n",
    "        for s in range(self.state_dimension):\n",
    "            action_circuits[s] = QuantumCircuit(self.action_qregister_size, name='action_s{}'.format(s)) # construct the quanutm circuit\n",
    "        for circuit in action_circuits:\n",
    "            circuit.h(list(range(self.action_qregister_size))) # apply H gate to every qubit register to create the equally weighted superposition\n",
    "        return action_circuits\n",
    "\n",
    "    def _init_grover_operators(self):\n",
    "        ''' \n",
    "        Initialize the grover operators of every action. U_grover := U_a0 * Ua where a0 is the equally superposition of all action eigenfunctions and a is an action eigenfunction. In fact,\n",
    "        U_grover is not updated during the training process within the scope of this project.\n",
    "        '''\n",
    "        grove_operators = np.zeros(self.action_dimension)\n",
    "        target_states = np.zeros(self.action_dimension)\n",
    "        for i in range(self.action_dimension):\n",
    "            state_binary = format(i, '0{}b'.format(self.action_qregister_size)) # generate the statevector binary string for encoding every action using the quantum register\n",
    "            grove_operators[i] = GroverOperator(oracle=Statevector.from_label(state_binary)).to_instruction()\n",
    "        return grove_operators\n",
    "\n",
    "    def _take_action(self):\n",
    "        ''' \n",
    "        Take an action under the current state by measuring the corresponding action wavefunction\n",
    "        '''\n",
    "        circuit = self.action_circuits[self.state] # the quanutm circuit encoding the up-to-date action wavefunction of the current state\n",
    "        circuit_to_measure = circuit.copy() # make a copy of the circuit for measurement so that the original circuit is not broken by the measurement\n",
    "        circuit_to_measure.measure_all() # take the action by measuring the current action wavefunciton\n",
    "        job = execute(circuit_to_measure, backend=self.backend, shots=1) # execute the circuit using the backend\n",
    "        result = job.result()\n",
    "        counts = result.get_counts()\n",
    "        action = int((list(counts.keys()))[0], 2) # take the action with highest probablity\n",
    "        return action\n",
    "\n",
    "    def _get_grover_length(self, reward, new_state):\n",
    "        ''' \n",
    "        Calculate length of the Grover iteration after taking an action\n",
    "        '''\n",
    "        k = self.hyperparameters['k']\n",
    "        return int(k * (reward + np.max(self.Q_values[new_state]))) # here we use max(Q_value[new_state]), it is also possible to use the expectation of Q_value[new_state] based on Born's rule \n",
    "        \n",
    "    def _run_grover_iterations(self):\n",
    "        '''\n",
    "        Run grover iterations within one learning step\n",
    "        '''\n",
    "        length = self.grover_length(self.state, self.action) # number of grover operators(iterations) to append in this steps\n",
    "        circuit = self.action_circuits(self.state) # the up-to-date quanutm circuit encodeing the action of current state\n",
    "        grover_operator = self.grover_operators(self.action) # the grover operator of current action\n",
    "\n",
    "    def _update_Q_values(self, reward, new_state):\n",
    "        alpha = self.hyperparameters['alpha']\n",
    "        gamma = self.hyperparameters['gamma']\n",
    "        self.Q_value[self.state, self.action] = self.Q_values[self.state, self.action] + alpha * (reward + gamma * np.max(self.Q_value[new_state]) - self.Q_values[self.state, self.action])\n",
    "\n",
    "    # train max epochs\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('qiskitenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17d49cc28dac616a7863004e14b04f7102b074af8657a84413b0a8fb3597af77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
