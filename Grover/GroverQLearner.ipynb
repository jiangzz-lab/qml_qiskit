{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'qiskit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqiskit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqiskit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcircuit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlibrary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GroverOperator\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqiskit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantum_info\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Statevector\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'qiskit'"
     ]
    }
   ],
   "source": [
    "from qiskit import *\n",
    "from qiskit.circuit.library import GroverOperator\n",
    "from qiskit.quantum_info import Statevector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.visualization import plot_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xmlrpc.client import boolean\n",
    "\n",
    "\n",
    "class GroverQLearner:\n",
    "    ''' \n",
    "    Implement a quanutm reinforcement learning agent based on Grover amplitute enhancement and QLearing algorithm.\n",
    "\n",
    "    Assumption:\n",
    "    The dimensions of the state space and action space are both finite\n",
    "\n",
    "    Parameters:\n",
    "    env: the environment to solve; default is OpenAI gym \"FrozenLake\"\n",
    "    state (int): current state\n",
    "    action (int): current action \n",
    "    state_dimension (int): dimension of the state space\n",
    "    action_dimension (int): dimension of the action space\n",
    "    action_qregister_size (int): number of qubits on the quantum register for storing the action wavefunction \n",
    "    max_grover_length (int): maximum of the length of the grover iteration\n",
    "    Q_values (2D np array): Q values of all (state, action) combinations; shape = (state_dimension, action_dimention)\n",
    "    grover_lengths (2D np array): lengths of grover iterations of all (state, action) combinaitons; shape = (state_dimension, action_dimention)\n",
    "    grover_operators (1D np array): grover_operators for all actions; grover_operators[a] records the grover operator constructed from action eigenfucntion a \n",
    "    action_circuits (1D np array): action quantum circuits for all states; action_circuits[s] records the quanutm circuit encoding the up-to-date action wavefunction of state s\n",
    "    hyperparameters (dict): hyperparameters of learning; \n",
    "                            {\n",
    "                                'k': prefactor of max grover length, \n",
    "                                'alpha': learning rate, 'gamma': discount, \n",
    "                                'epsilonr': tolerance of the Q values,\n",
    "                                'max_epochs': max number of epochs for training,\n",
    "                                'max_steps': max number of steps in every epoch\n",
    "                            }\n",
    "    backend: machine to execute the quanutm circuit jobs; could be either a simulator or a true quantum computer\n",
    "    '''\n",
    "    def __init__(self, env) -> None:\n",
    "        self.env = env\n",
    "        self.state = env.reset()\n",
    "        self.action = 0\n",
    "        self.state_dimension = env.observation_space.n\n",
    "        self.action_dimension = env.action_space.n\n",
    "        self.action_qregister_size = ceil(np.log2(self.action_dimension))\n",
    "        self.max_grover_length = int(round(np.pi / (4 * np.arcsin(1. / np.sqrt(2 ** self.action_qregister_size))) - 0.5))\n",
    "        self.Q_values = np.zeros((self.state_dimension, self.action_dimension), dtype=float) \n",
    "        self.grover_lengths = np.zeros((self.state_dimension, self.action_dimension), dtype=int)\n",
    "        self.max_grover_length_reached = np.zeros((self.state_dimension, self.action_dimension), dtype=boolean)\n",
    "        self.grover_operators = self._init_grover_operators()\n",
    "        self.action_circuits = self._init_action_circuits()\n",
    "        self.hyperparameters = {\n",
    "            'k': 0.1,\n",
    "            'alpha': 0.05,\n",
    "            'gamma': 0.99,\n",
    "            'eps': 0.01,\n",
    "            'max_epochs': 1000,\n",
    "            'max_steps': 100\n",
    "        }\n",
    "        self.backend = Aer.get_backend('qasm_simulator')\n",
    "            \n",
    "    # hyperparameter setter\n",
    "    def set_hyperparameters(self, params):\n",
    "        pass\n",
    "\n",
    "    def _init_action_circuits(self):\n",
    "        '''\n",
    "        Initialize the quanutm circuits encoding the action wavefunction of every state. Every initial action wavefunction is a equally weighted superposition of all action eignenfucntions. \n",
    "        '''\n",
    "        action_circuits = np.zeros(self.state_dimension)\n",
    "        for s in range(self.state_dimension):\n",
    "            action_circuits[s] = QuantumCircuit(self.action_qregister_size, name='action_s{}'.format(s)) # construct the quanutm circuit\n",
    "        for circuit in action_circuits:\n",
    "            circuit.h(list(range(self.action_qregister_size))) # apply H gate to every qubit register to create the equally weighted superposition\n",
    "        return action_circuits\n",
    "\n",
    "    def _init_grover_operators(self):\n",
    "        ''' \n",
    "        Initialize the grover operators of every action. U_grover := U_a0 * Ua where a0 is the equally superposition of all action eigenfunctions and a is an action eigenfunction. In fact,\n",
    "        U_grover is not updated during the training process within the scope of this project.\n",
    "        '''\n",
    "        grove_operators = np.zeros(self.action_dimension)\n",
    "        target_states = np.zeros(self.action_dimension)\n",
    "        for i in range(self.action_dimension):\n",
    "            state_binary = format(i, '0{}b'.format(self.action_qregister_size)) # generate the statevector binary string for encoding every action using the quantum register\n",
    "            grove_operators[i] = GroverOperator(oracle=Statevector.from_label(state_binary)).to_instruction()\n",
    "        return grove_operators\n",
    "\n",
    "    def _take_action(self):\n",
    "        ''' \n",
    "        Take an action under the current state by measuring the corresponding action wavefunction\n",
    "        '''\n",
    "        circuit = self.action_circuits[self.state] # the quanutm circuit encoding the up-to-date action wavefunction of the current state\n",
    "        circuit_to_measure = circuit.copy() # make a copy of the circuit for measurement so that the original circuit is not broken by the measurement\n",
    "        circuit_to_measure.measure_all() # take the action by measuring the current action wavefunciton\n",
    "        job = execute(circuit_to_measure, backend=self.backend, shots=1) # execute the circuit using the backend\n",
    "        result = job.result()\n",
    "        counts = result.get_counts()\n",
    "        action = int((list(counts.keys()))[0], 2) # take the action with highest probablity\n",
    "        return action\n",
    "\n",
    "    def _get_grover_length(self, reward, new_state):\n",
    "        ''' \n",
    "        Calculate length of the Grover iteration after taking an action\n",
    "        '''\n",
    "        k = self.hyperparameters['k']\n",
    "        return int(k * (reward + np.max(self.Q_values[new_state]))) # here we use max(Q_value[new_state]), it is also possible to use the expectation of Q_value[new_state] based on Born's rule \n",
    "        \n",
    "    def _run_grover_iterations(self):\n",
    "        '''\n",
    "        Run grover iterations at one state\n",
    "        '''\n",
    "        length = min(self.grover_length(self.state, self.action), self.max_grover_length) # number of grover operators(iterations) to append in this steps\n",
    "        circuit = self.action_circuits(self.state) # the up-to-date quanutm circuit encodeing the action of current state\n",
    "        grover_operator = self.grover_operators(self.action) # the grover operator of current action\n",
    "        max_grover_length_reached = self.max_grover_length_reached(self.state)\n",
    "        if not(max_grover_length_reached.any()):\n",
    "            for _ in length:\n",
    "                circuit.append(grover_operator, list(range(self.action_qregister_size)))\n",
    "        if length == self.max_grover_length and (not(max_grover_length_reached.any())): \n",
    "            self.max_grover_length_reached[self.state, self.action] = True  # update the self.max_grover_length_reached when the max grove length is reached\n",
    "        self.action_circuits[self.state] = circuit # update the circuit\n",
    "\n",
    "    def _update_Q_values(self, reward, new_state):\n",
    "        ''' \n",
    "        Update the Q_value after one state transition\n",
    "        '''\n",
    "        alpha = self.hyperparameters['alpha']\n",
    "        gamma = self.hyperparameters['gamma']\n",
    "        self.Q_value[self.state, self.action] = self.Q_values[self.state, self.action] + alpha * (reward + gamma * np.max(self.Q_value[new_state]) - self.Q_values[self.state, self.action])\n",
    "\n",
    "    def train(self):\n",
    "        ''' \n",
    "        Train the GroverQlearner agent by running multiple epochs with the given max epoch.\n",
    "        Record the step used in each epoch, whether the target is reached, and the trajectory\n",
    "        '''\n",
    "        eps = self.hyperparameters['eps'] # the Q_value table tolerance\n",
    "        max_epochs = self.hyperparameters['max_epochs']\n",
    "        optimal_steps = self.hyperparameters['max_steps'] # initial the steps to be the max steps\n",
    "        target_reached = False\n",
    "        trajectory = []\n",
    "        steps_in_all_epochs = []\n",
    "        target_reached_in_all_epochs = []\n",
    "        trajectories_in_all_epochs = [] # stores <epoch name(string), state history(list of state)>\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"Processing epoch {} ...\".format(epoch)) # monitor the training process\n",
    "            self.state = self.env.reset() # reset env\n",
    "            target_reached = False # init target reached flag\n",
    "            trajectory = [self.state] # list to record the trajectory of the current epoch\n",
    "            \n",
    "            for step in range(optimal_steps): # take steps\n",
    "                print('Taking step {0}/{1}'.format(step, optimal_steps), end='\\r')\n",
    "                self.action = self._take_action() # take an action\n",
    "                new_state, reward, _, done, _ = self.env.step(self.action) # step to the new state\n",
    "                if new_state == self.state:\n",
    "                    reward -= 10 # take less reward if stay in the same state\n",
    "                    done = True # no need to step more if the state cannot be changed\n",
    "                if new_state == self.state_dimension - 1:\n",
    "                    reward += 99 # take very large reward when the target is reached\n",
    "                    optimal_steps = step + 1 # update the optimal steps when the target is reached\n",
    "                    target_reached = True\n",
    "                elif not done: \n",
    "                    reward -= 1  # when the state is changed but the target is not reached, take a moderate reward\n",
    "                self._update_Q_values(reward, new_state)\n",
    "                self.grover_lengths[self.state, self.action] = self._get_grover_length(reward, new_state) # get grover length\n",
    "                self._run_grover_iterations() # run grover iterations to tune the amplitude of the action eigenfunctions\n",
    "                trajectory.append(new_state) # append the new state to the trajectory\n",
    "                if done:\n",
    "                    break\n",
    "                self.state = new_state # update the state if it is changed\n",
    "\n",
    "            steps_in_all_epochs.append[optimal_steps]\n",
    "            target_reached_in_all_epochs.append[target_reached]\n",
    "            trajectories_in_all_epochs.append[trajectory]\n",
    "\n",
    "        return steps_in_all_epochs, target_reached_in_all_epochs, trajectories_in_all_epochs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('qiskitenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e641ae9ccb54d7232dedb8d3a03db68372cdec318f878be8e32a7b205edf548c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
